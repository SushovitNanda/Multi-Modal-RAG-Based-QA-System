{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Modal RAG QA System (single-file)\n",
    "- Ingestion: PDFs, DOCX, images (OCR), tables, chart metadata\n",
    "- Hybrid IR: TF-IDF + Word2Vec + SBERT(query) + Cross-Encoder rerank\n",
    "- Vector DB: FAISS (faiss-cpu) for dense embeddings (SBERT)\n",
    "- UI: Streamlit chatbot + retrieval debugging\n",
    "\n",
    "Author: ChatGPT (adapted to your assignment)\n",
    "Assignment doc path (local): /mnt/data/multi-modal_rag_qa_assignment.docx\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain loaders & splitters\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_document_loaders import UnstructuredPDFLoader  # optional\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ingestion libs\n",
    "import pdfplumber\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import camelot\n",
    "\n",
    "# Embeddings & IR\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "# FAISS vector store\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# LangChain LLM chain (for answer generation)\n",
    "from langchain.llms import OpenAI  # or use a local LLM wrapper\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import OpenAI as LCOpenAI\n",
    "\n",
    "# UI\n",
    "import streamlit as st\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "RAW_DOC_DIR = \"./data/raw\"            # folder where user PDFs/docs live\n",
    "PROCESSED_DIR = \"./data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Path to the assignment doc (developer instruction insisted this path be shown)\n",
    "ASSIGNMENT_DOC_LOCAL_PATH = \"/mnt/data/multi-modal_rag_qa_assignment.docx\"\n",
    "\n",
    "# Filenames for outputs\n",
    "PROCESSED_JSON = os.path.join(PROCESSED_DIR, \"processed_chunks.json\")\n",
    "TFIDF_MODEL_PATH = os.path.join(PROCESSED_DIR, \"tfidf_vectorizer.pkl\")\n",
    "FAISS_INDEX_PATH = os.path.join(PROCESSED_DIR, \"faiss_index\")\n",
    "SBERT_EMB_PATH = os.path.join(PROCESSED_DIR, \"sbert_doc_embeddings.npy\")\n",
    "WORD2VEC_NAME = \"word2vec-google-news-300\"  # may be large; fallback allowed\n",
    "\n",
    "# Embedding & retrieval hyperparams\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K_CANDIDATES = 50\n",
    "TOP_K_FINAL = 5\n",
    "\n",
    "# Hybrid scoring weights (tune these)\n",
    "W_TFIDF = 0.6\n",
    "W_W2V = 0.2\n",
    "W_SQ = 0.2  # SBERT(query) weight for initial score (used as tie-breaker)\n",
    "\n",
    "# LLM Settings (set your keys in env if using OpenAI)\n",
    "LLM_PROVIDER = os.environ.get(\"LLM_PROVIDER\", \"openai\")  # or \"local\"\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "# Model names (sentence-transformers)\n",
    "SBERT_MODEL_NAME = \"all-mpnet-base-v2\"   # dense encoder for docs/queries\n",
    "CROSS_ENCODER_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # for reranking\n",
    "\n",
    "# Device\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------\n",
    "# UTIL: Save / Load helpers\n",
    "# -----------------------\n",
    "import pickle\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# -----------------------\n",
    "# INGESTION: text, tables, images, OCR, chart metadata\n",
    "# -----------------------\n",
    "def extract_text_with_langchain(pdf_path: str):\n",
    "    \"\"\"Use LangChain's PyMuPDFLoader (robust) to extract text pages.\"\"\"\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()  # returns list of Document objects with metadata including 'page'\n",
    "    # Convert to simple dicts: {\"page\": int, \"text\": str}\n",
    "    page_texts = []\n",
    "    for d in docs:\n",
    "        page_num = d.metadata.get(\"page\")\n",
    "        page_texts.append({\"page\": page_num, \"text\": d.page_content})\n",
    "    return page_texts\n",
    "\n",
    "def extract_tables_pdf(pdf_path: str):\n",
    "    \"\"\"Extract tables using camelot (works on vector PDFs). Returns list of dicts.\"\"\"\n",
    "    table_records = []\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')  # try lattice first\n",
    "    except Exception:\n",
    "        tables = []\n",
    "    # fallback to stream flavor if lattice yields nothing\n",
    "    if len(tables) == 0:\n",
    "        try:\n",
    "            tables = camelot.read_pdf(pdf_path, pages='all', flavor='stream')\n",
    "        except Exception:\n",
    "            tables = []\n",
    "    for t in tables:\n",
    "        try:\n",
    "            df = t.df\n",
    "            page = t.page\n",
    "            table_text = df.to_csv(index=False)\n",
    "            table_records.append({\"page\": int(page), \"table_text\": table_text})\n",
    "        except Exception:\n",
    "            continue\n",
    "    return table_records\n",
    "\n",
    "def extract_images_and_ocr(pdf_path: str, output_dir: str) -> List[Dict[str,Any]]:\n",
    "    \"\"\"Extract embedded images using PyMuPDF and run OCR (pytesseract).\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for pno in range(len(pdf)):\n",
    "        page = pdf[pno]\n",
    "        page_images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(page_images):\n",
    "            xref = img[0]\n",
    "            base_image = pdf.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            img_ext = base_image.get(\"ext\", \"png\")\n",
    "            img_name = f\"{Path(pdf_path).stem}_p{pno+1}_img{img_index+1}.{img_ext}\"\n",
    "            img_path = os.path.join(output_dir, img_name)\n",
    "            with open(img_path, \"wb\") as imf:\n",
    "                imf.write(image_bytes)\n",
    "            # Preprocess image (optional): convert to grayscale, threshold etc.\n",
    "            try:\n",
    "                pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                # OCR\n",
    "                ocr_text = pytesseract.image_to_string(pil_img)\n",
    "            except Exception:\n",
    "                ocr_text = \"\"\n",
    "            images.append({\"page\": pno+1, \"image_path\": img_path, \"ocr_text\": ocr_text})\n",
    "    return images\n",
    "\n",
    "def extract_chart_metadata_from_page_text(text: str) -> Dict[str,str]:\n",
    "    \"\"\"Attempt to find chart captions / figure captions / axis labels via heuristics.\"\"\"\n",
    "    # Very simple heuristics: look for lines starting with 'Figure' or 'Fig.' or 'Chart'\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    captions = []\n",
    "    for ln in lines[:50]:  # first 50 lines more likely include caption/heading\n",
    "        if ln.lower().startswith((\"figure\", \"fig.\", \"chart\", \"table\")):\n",
    "            captions.append(ln)\n",
    "    # also capture lines containing words 'axis', 'x-axis', 'y-axis', 'legend' as potential metadata\n",
    "    for ln in lines:\n",
    "        if any(k in ln.lower() for k in [\"axis\", \"x-axis\", \"y-axis\", \"legend\", \"units\", \"scale\"]):\n",
    "            captions.append(ln)\n",
    "    return {\"captions\": \" | \".join(captions)}\n",
    "\n",
    "# -----------------------\n",
    "# PREPROCESS & CHUNKING\n",
    "# -----------------------\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # minimal cleaning: normalize whitespace\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def chunk_documents(page_records: List[Dict[str,Any]], table_records: List[Dict[str,Any]],\n",
    "                    image_records: List[Dict[str,Any]], chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Build unified chunks with metadata.\n",
    "    For each page: chunk page text; add table rows as chunks; add image OCR as chunks.\n",
    "    Returns list of dicts: {id, type, page, content, metadata}\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap,\n",
    "                                              separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"])\n",
    "    docs_for_split = []\n",
    "    meta_map = []\n",
    "    # page texts\n",
    "    for rec in page_records:\n",
    "        pg = rec[\"page\"]\n",
    "        txt = clean_text(rec[\"text\"])\n",
    "        if not txt:\n",
    "            continue\n",
    "        docs_for_split.append(txt)\n",
    "        meta_map.append({\"type\": \"page_text\", \"page\": pg, \"source\": rec.get(\"source\", None)})\n",
    "    # tables as text\n",
    "    for trec in table_records:\n",
    "        pg = trec[\"page\"]\n",
    "        ttxt = clean_text(trec[\"table_text\"])\n",
    "        if ttxt:\n",
    "            docs_for_split.append(ttxt)\n",
    "            meta_map.append({\"type\": \"table\", \"page\": pg})\n",
    "    # image OCR texts appended as documents for splitting too\n",
    "    for irec in image_records:\n",
    "        pg = irec.get(\"page\")\n",
    "        ocr = clean_text(irec.get(\"ocr_text\", \"\"))\n",
    "        if ocr:\n",
    "            docs_for_split.append(ocr)\n",
    "            meta_map.append({\"type\": \"image_ocr\", \"page\": pg, \"image_path\": irec.get(\"image_path\")})\n",
    "    # now use splitter to split each doc separately and produce chunk metadata\n",
    "    all_chunks = []\n",
    "    chunk_id = 0\n",
    "    for i, full_text in enumerate(docs_for_split):\n",
    "        # naive Document wrapper for splitter: it expects docs with page_content + metadata; we simulate\n",
    "        dummy_doc = type(\"D\", (), {})()\n",
    "        dummy_doc.page_content = full_text\n",
    "        dummy_doc.metadata = meta_map[i]\n",
    "        splitted = splitter.split_documents([dummy_doc])\n",
    "        for s in splitted:\n",
    "            all_chunks.append({\n",
    "                \"id\": f\"chunk_{chunk_id}\",\n",
    "                \"type\": s.metadata.get(\"type\"),\n",
    "                \"page\": s.metadata.get(\"page\"),\n",
    "                \"content\": s.page_content,\n",
    "                \"metadata\": s.metadata\n",
    "            })\n",
    "            chunk_id += 1\n",
    "    return all_chunks\n",
    "\n",
    "# -----------------------\n",
    "# EMBEDDINGS & INDEX BUILDING\n",
    "# -----------------------\n",
    "def build_tfidf(doc_texts: List[str]) -> Tuple[TfidfVectorizer, Any]:\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=\"english\", max_features=50000)\n",
    "    X = tfidf.fit_transform(doc_texts)\n",
    "    return tfidf, X\n",
    "\n",
    "def load_word2vec_model(name=WORD2VEC_NAME):\n",
    "    try:\n",
    "        print(\"Loading Word2Vec (this can take time & memory)...\")\n",
    "        w2v = gensim_api.load(name)\n",
    "        print(\"Word2Vec loaded.\")\n",
    "        return w2v\n",
    "    except Exception as e:\n",
    "        print(\"Word2Vec not available:\", e)\n",
    "        return None\n",
    "\n",
    "def get_avg_word2vec_embeddings(w2v_model, texts: List[str]) -> np.ndarray:\n",
    "    if w2v_model is None:\n",
    "        return np.zeros((len(texts), 300))\n",
    "    embs = []\n",
    "    for t in texts:\n",
    "        words = [w for w in t.split() if w]\n",
    "        vecs = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                vecs.append(w2v_model[w.lower()])\n",
    "            except Exception:\n",
    "                continue\n",
    "        if vecs:\n",
    "            embs.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            embs.append(np.zeros(w2v_model.vector_size))\n",
    "    return np.vstack(embs)\n",
    "\n",
    "def build_sbert_and_faiss(texts: List[str], model_name=SBERT_MODEL_NAME, faiss_index_path=FAISS_INDEX_PATH):\n",
    "    # SentenceTransformer to get dense vectors; then build FAISS index\n",
    "    sbert = SentenceTransformer(model_name, device=DEVICE)\n",
    "    doc_embs = sbert.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    # use faiss IndexFlatIP with normalized vectors (cosine sim)\n",
    "    dim = doc_embs.shape[1]\n",
    "    print(f\"Building FAISS index (dim={dim})\")\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    # normalize embeddings\n",
    "    faiss.normalize_L2(doc_embs)\n",
    "    index.add(doc_embs)\n",
    "    # save index and embeddings\n",
    "    if not os.path.exists(faiss_index_path):\n",
    "        os.makedirs(faiss_index_path, exist_ok=True)\n",
    "    faiss.write_index(index, os.path.join(faiss_index_path, \"index.faiss\"))\n",
    "    np.save(os.path.join(faiss_index_path, \"doc_embs.npy\"), doc_embs)\n",
    "    return sbert, index, doc_embs\n",
    "\n",
    "# -----------------------\n",
    "# HYBRID RETRIEVAL\n",
    "# -----------------------\n",
    "def hybrid_retrieval(query: str,\n",
    "                     tfidf_vectorizer: TfidfVectorizer, tfidf_matrix,\n",
    "                     w2v_model, w2v_doc_embs,\n",
    "                     sbert_model: SentenceTransformer, sbert_doc_embs,\n",
    "                     doc_texts: List[str], top_k=TOP_K_CANDIDATES):\n",
    "    \"\"\"\n",
    "    Return top_k candidate doc indices and fused scores.\n",
    "    Steps:\n",
    "      - compute TF-IDF similarity (cosine) between query and docs\n",
    "      - compute avg Word2Vec similarity (if available)\n",
    "      - compute SBERT (query) dense similarity (cosine via prenormalized vectors)\n",
    "      - normalize the three scores and compute weighted sum\n",
    "    \"\"\"\n",
    "    # TF-IDF\n",
    "    q_tfidf = tfidf_vectorizer.transform([query])\n",
    "    tfidf_sims = cosine_similarity(q_tfidf, tfidf_matrix).reshape(-1)  # shape (n_docs,)\n",
    "\n",
    "    # Word2Vec\n",
    "    if w2v_model is not None and w2v_doc_embs is not None:\n",
    "        q_w2v = get_avg_word2vec_embeddings(w2v_model, [query])[0].reshape(1, -1)\n",
    "        # handle zero vectors\n",
    "        if np.linalg.norm(q_w2v) == 0:\n",
    "            w2v_sims = np.zeros(len(w2v_doc_embs))\n",
    "        else:\n",
    "            w2v_sims = cosine_similarity(q_w2v, w2v_doc_embs).reshape(-1)\n",
    "    else:\n",
    "        w2v_sims = np.zeros(len(doc_texts))\n",
    "\n",
    "    # SBERT query\n",
    "    q_sbert = sbert_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_sbert)\n",
    "    # use dot product with pre-normalized sbert_doc_embs\n",
    "    q_norm = q_sbert / np.linalg.norm(q_sbert)\n",
    "    sbert_sims = np.dot(sbert_doc_embs, q_norm.reshape(-1)).reshape(-1)\n",
    "\n",
    "    # Normalize component scores to [0,1]\n",
    "    def norm01(x):\n",
    "        if x.max() - x.min() <= 1e-9:\n",
    "            return np.zeros_like(x)\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "    tfidf_norm = norm01(tfidf_sims)\n",
    "    w2v_norm = norm01(w2v_sims)\n",
    "    sbert_norm = norm01(sbert_sims)\n",
    "\n",
    "    fused = W_TFIDF * tfidf_norm + W_W2V * w2v_norm + W_SQ * sbert_norm\n",
    "\n",
    "    top_idxs = np.argsort(fused)[::-1][:top_k]\n",
    "    top_scores = fused[top_idxs]\n",
    "    return top_idxs, top_scores, {\"tfidf\": tfidf_norm, \"w2v\": w2v_norm, \"sbert\": sbert_norm, \"fused\": fused}\n",
    "\n",
    "# -----------------------\n",
    "# CROSS-ENCODER RERANK\n",
    "# -----------------------\n",
    "def cross_encoder_rerank(query: str, candidate_texts: List[str], cross_encoder_model: CrossEncoder, top_k=TOP_K_FINAL):\n",
    "    \"\"\"\n",
    "    Cross-encoder expects list of (query, passage) pairs.\n",
    "    Returns top_k indices and cross-encoder scores (higher better).\n",
    "    \"\"\"\n",
    "    pairs = [[query, t] for t in candidate_texts]\n",
    "    scores = cross_encoder_model.predict(pairs, show_progress_bar=False)\n",
    "    order = np.argsort(scores)[::-1][:top_k]\n",
    "    return order, scores[order]\n",
    "\n",
    "# -----------------------\n",
    "# QA GENERATION (LLM)\n",
    "# -----------------------\n",
    "def make_retrieval_qa_chain(llm_model_name_or_instance, embedding_retriever):\n",
    "    \"\"\"\n",
    "    Make a LangChain RetrievalQA chain using the given LLM and retriever.\n",
    "    embedding_retriever should implement get_relevant_documents(query) -> list(Document)\n",
    "    We'll use LangChain's RetrievalQA wrapper with a prompt that enforces citations.\n",
    "    \"\"\"\n",
    "    # LLM wrapper: use OpenAI or LangChain OpenAI wrapper\n",
    "    if LLM_PROVIDER == \"openai\" and OPENAI_API_KEY:\n",
    "        llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "    else:\n",
    "        # fallback to LangChain OpenAI wrapper (requires env vars)\n",
    "        llm = LCOpenAI(temperature=0)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "        template=(\n",
    "            \"You are a helpful assistant. Use ONLY the provided context to answer the question. \"\n",
    "            \"Cite sources inline by specifying (page: X) or (table: page X). If the answer is not in the context, say 'I don't know'.\\n\\n\"\n",
    "            \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "        )\n",
    "    )\n",
    "    # RetrievalQA wrapper expects a Retriever; we can pass our own retriever object (wrap hybrid)\n",
    "    from langchain.chains import RetrievalQA\n",
    "    return RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=embedding_retriever, return_source_documents=True)\n",
    "\n",
    "# -----------------------\n",
    "# EVALUATION METRICS (MRR, NDCG, P/R/F)\n",
    "# -----------------------\n",
    "def calculate_ir_metrics_for_query(similarities: np.ndarray, query_label_indices: np.ndarray, labels_mat: np.ndarray, k=5):\n",
    "    \"\"\"\n",
    "    Compute metrics for one query using the same logic as user's earlier functions.\n",
    "    similarities: vector (n_docs,) ranking scores (higher better)\n",
    "    query_label_indices: index(es) of documents representing the query's label (e.g., doc indices belonging to same page)\n",
    "    labels_mat: (n_docs, n_labels) binary label matrix\n",
    "    \"\"\"\n",
    "    ranked = np.argsort(similarities)[::-1]\n",
    "    # Build relevant set: any doc sharing a label with the query docs\n",
    "    relevant_set = set()\n",
    "    # for each representative doc index in query_label_indices, add docs sharing same labels\n",
    "    for qidx in query_label_indices:\n",
    "        if qidx < 0: continue\n",
    "        qlabels = np.where(labels_mat[qidx] == 1)[0]\n",
    "        for lab in qlabels:\n",
    "            relevant = set(np.where(labels_mat[:, lab] == 1)[0].tolist())\n",
    "            relevant_set.update(relevant)\n",
    "    # remove query docs if present\n",
    "    for qidx in query_label_indices:\n",
    "        if qidx in relevant_set: relevant_set.remove(qidx)\n",
    "    if not relevant_set:\n",
    "        return {\"mrr\": 0.0, \"ndcg\": 0.0, \"ndcg_at_k\": 0.0, \"precision\": 0.0,\n",
    "                \"recall\": 0.0, \"f1\": 0.0, \"precision_at_k\": 0.0, \"recall_at_k\": 0.0, \"f1_at_k\": 0.0}\n",
    "    # compute top-k\n",
    "    top_k = ranked[:k]\n",
    "    retrieved_relevant_k = len(set(top_k) & relevant_set)\n",
    "    precision_k = retrieved_relevant_k / k\n",
    "    recall_k = retrieved_relevant_k / len(relevant_set)\n",
    "    f1_k = 2 * precision_k * recall_k / (precision_k + recall_k) if (precision_k + recall_k) > 0 else 0.0\n",
    "    # overall consider first min(len(ranked), len(relevant_set)*2)\n",
    "    cutoff = min(len(ranked), max(1, len(relevant_set)*2))\n",
    "    retrieved_docs = ranked[:cutoff]\n",
    "    retrieved_relevant = len(set(retrieved_docs) & relevant_set)\n",
    "    precision = retrieved_relevant / cutoff if cutoff > 0 else 0.0\n",
    "    recall = retrieved_relevant / len(relevant_set)\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    # MRR: position of first relevant in top-k\n",
    "    mrr = 0.0\n",
    "    for rank_idx, doc_idx in enumerate(top_k, 1):\n",
    "        if doc_idx in relevant_set:\n",
    "            mrr = 1.0 / rank_idx\n",
    "            break\n",
    "    # NDCG@K: relevance = 1 if relevant else 0 (binary)\n",
    "    dcg = 0.0\n",
    "    for i, doc_idx in enumerate(top_k, 1):\n",
    "        rel = 1 if doc_idx in relevant_set else 0\n",
    "        dcg += rel / math.log2(i+1)\n",
    "    ideal_dcg = sum(1.0 / math.log2(i+1) for i in range(1, min(len(relevant_set), k)+1))\n",
    "    ndcg_k = dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "    # overall NDCG (cutoff 100)\n",
    "    cutoff_overall = min(len(ranked), 100)\n",
    "    dcg_overall = 0.0\n",
    "    for i, doc_idx in enumerate(ranked[:cutoff_overall], 1):\n",
    "        rel = 1 if doc_idx in relevant_set else 0\n",
    "        dcg_overall += rel / math.log2(i+1)\n",
    "    ideal_overall = sum(1.0 / math.log2(i+1) for i in range(1, min(len(relevant_set), cutoff_overall)+1))\n",
    "    ndcg_overall = dcg_overall / ideal_overall if ideal_overall > 0 else 0.0\n",
    "    return {\"mrr\": mrr, \"ndcg\": ndcg_overall, \"ndcg_at_k\": ndcg_k, \"precision\": precision,\n",
    "            \"recall\": recall, \"f1\": f1, \"precision_at_k\": precision_k, \"recall_at_k\": recall_k, \"f1_at_k\": f1_k}\n",
    "\n",
    "# -----------------------\n",
    "# MAIN: Build pipeline, index, and launch Streamlit UI\n",
    "# -----------------------\n",
    "def build_pipeline_and_index(rebuild=False):\n",
    "    \"\"\"\n",
    "    Runs ingestion -> chunking -> embeddings -> indices.\n",
    "    Saves processed chunks and indexes to disk for reuse.\n",
    "    \"\"\"\n",
    "    # If files exist and not rebuilding, load them\n",
    "    if (os.path.exists(PROCESSED_JSON) and os.path.exists(os.path.join(FAISS_INDEX_PATH, \"index.faiss\"))\n",
    "        and os.path.exists(SBERT_EMB_PATH) and not rebuild):\n",
    "        print(\"Loading existing processed chunks and FAISS index...\")\n",
    "        with open(PROCESSED_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks = json.load(f)\n",
    "        index = faiss.read_index(os.path.join(FAISS_INDEX_PATH, \"index.faiss\"))\n",
    "        doc_embs = np.load(os.path.join(FAISS_INDEX_PATH, \"doc_embs.npy\"))\n",
    "        # load tfidf and w2v if saved\n",
    "        tfidf = None\n",
    "        try:\n",
    "            tfidf = load_pickle(TFIDF_MODEL_PATH)\n",
    "            tfidf_matrix = tfidf.transform([c[\"content\"] for c in chunks])\n",
    "        except Exception:\n",
    "            tfidf = None\n",
    "            tfidf_matrix = None\n",
    "        # attempt load word2vec doc emb\n",
    "        try:\n",
    "            w2v_doc_embs = np.load(os.path.join(PROCESSED_DIR, \"w2v_doc_embs.npy\"))\n",
    "        except Exception:\n",
    "            w2v_doc_embs = None\n",
    "        # load SBERT model\n",
    "        sbert = SentenceTransformer(SBERT_MODEL_NAME, device=DEVICE)\n",
    "        return chunks, tfidf, tfidf_matrix, None, w2v_doc_embs, sbert, doc_embs, index\n",
    "\n",
    "    # 1) Ingest documents from RAW_DOC_DIR\n",
    "    all_chunks = []\n",
    "    page_texts_all = []\n",
    "    table_records_all = []\n",
    "    image_records_all = []\n",
    "    # iterate files\n",
    "    files = [str(p) for p in Path(RAW_DOC_DIR).glob(\"*\") if p.suffix.lower() in [\".pdf\", \".docx\"]]\n",
    "    print(f\"Found {len(files)} raw documents in {RAW_DOC_DIR}\")\n",
    "    for fpath in files:\n",
    "        # extract page text using langchain loader\n",
    "        try:\n",
    "            page_texts = extract_text_with_langchain(fpath)\n",
    "        except Exception:\n",
    "            # fallback to pdfplumber\n",
    "            page_texts = []\n",
    "            try:\n",
    "                with pdfplumber.open(fpath) as pdf:\n",
    "                    for i, p in enumerate(pdf.pages):\n",
    "                        page_texts.append({\"page\": i+1, \"text\": p.extract_text()})\n",
    "            except Exception:\n",
    "                page_texts = []\n",
    "        # augment page data with source\n",
    "        for p in page_texts:\n",
    "            p[\"source\"] = fpath\n",
    "        page_texts_all.extend(page_texts)\n",
    "        # tables\n",
    "        table_records = extract_tables_pdf(fpath)\n",
    "        for t in table_records:\n",
    "            t[\"source\"] = fpath\n",
    "        table_records_all.extend(table_records)\n",
    "        # images + OCR\n",
    "        img_out_dir = os.path.join(PROCESSED_DIR, \"images\")\n",
    "        images = extract_images_and_ocr(fpath, img_out_dir)\n",
    "        for im in images:\n",
    "            im[\"source\"] = fpath\n",
    "        image_records_all.extend(images)\n",
    "\n",
    "    # 2) chunking\n",
    "    chunks = chunk_documents(page_texts_all, table_records_all, image_records_all,\n",
    "                             chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    print(f\"Created {len(chunks)} chunks.\")\n",
    "\n",
    "    # Save processed chunks\n",
    "    with open(PROCESSED_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2)\n",
    "\n",
    "    # Texts list for embedding\n",
    "    doc_texts = [c[\"content\"] for c in chunks]\n",
    "\n",
    "    # 3) TF-IDF\n",
    "    tfidf_vectorizer, tfidf_matrix = build_tfidf(doc_texts)\n",
    "    save_pickle(tfidf_vectorizer, TFIDF_MODEL_PATH)\n",
    "    # optionally save tfidf_matrix as sparse npz\n",
    "    import scipy.sparse\n",
    "    scipy.sparse.save_npz(os.path.join(PROCESSED_DIR, \"tfidf_matrix.npz\"), tfidf_matrix)\n",
    "\n",
    "    # 4) Word2Vec embeddings\n",
    "    w2v = load_word2vec_model(WORD2VEC_NAME)\n",
    "    if w2v is not None:\n",
    "        w2v_doc_embs = get_avg_word2vec_embeddings(w2v, doc_texts)\n",
    "        np.save(os.path.join(PROCESSED_DIR, \"w2v_doc_embs.npy\"), w2v_doc_embs)\n",
    "    else:\n",
    "        w2v_doc_embs = None\n",
    "\n",
    "    # 5) SBERT & FAISS\n",
    "    sbert_model, faiss_index, sbert_doc_embs = build_sbert_and_faiss(doc_texts, model_name=SBERT_MODEL_NAME, faiss_index_path=FAISS_INDEX_PATH)\n",
    "\n",
    "    # Save doc texts & chunks mapping\n",
    "    with open(os.path.join(PROCESSED_DIR, \"doc_texts.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc_texts, f, indent=2)\n",
    "\n",
    "    return chunks, tfidf_vectorizer, tfidf_matrix, w2v, w2v_doc_embs, sbert_model, sbert_doc_embs, faiss_index\n",
    "\n",
    "# -----------------------\n",
    "# STREAMLIT APP\n",
    "# -----------------------\n",
    "def run_streamlit_app():\n",
    "    st.set_page_config(page_title=\"Multi-Modal RAG QA\", layout=\"wide\")\n",
    "    st.title(\"Multi-Modal RAG QA — Chatbot (Hybrid IR + Cross-Encoder Rerank)\")\n",
    "    st.markdown(\"**Assignment doc (local path):** \" + ASSIGNMENT_DOC_LOCAL_PATH)\n",
    "\n",
    "    # sidebar controls\n",
    "    st.sidebar.header(\"Configuration\")\n",
    "    rebuild = st.sidebar.checkbox(\"Rebuild index & processed data\", value=False)\n",
    "    topk_candidates = st.sidebar.number_input(\"Candidate retrieval K\", value=TOP_K_CANDIDATES, min_value=10, max_value=500, step=10)\n",
    "    topk_final = st.sidebar.number_input(\"Final top-K after rerank\", value=TOP_K_FINAL, min_value=1, max_value=20)\n",
    "    run_build = st.sidebar.button(\"Build / Load pipeline\")\n",
    "\n",
    "    if run_build or \"pipeline_built\" not in st.session_state:\n",
    "        with st.spinner(\"Building pipeline (ingestion, chunking, embeddings, FAISS). This may take some minutes...\"):\n",
    "            chunks, tfidf_vec, tfidf_mat, w2v_model, w2v_doc_embs, sbert_model, sbert_doc_embs, faiss_index = build_pipeline_and_index(rebuild=rebuild)\n",
    "            st.session_state[\"chunks\"] = chunks\n",
    "            st.session_state[\"tfidf_vec\"] = tfidf_vec\n",
    "            st.session_state[\"tfidf_mat\"] = tfidf_mat\n",
    "            st.session_state[\"w2v_model\"] = w2v_model\n",
    "            st.session_state[\"w2v_doc_embs\"] = w2v_doc_embs\n",
    "            st.session_state[\"sbert_model\"] = sbert_model\n",
    "            st.session_state[\"sbert_doc_embs\"] = sbert_doc_embs\n",
    "            st.session_state[\"faiss_index\"] = faiss_index\n",
    "            st.session_state[\"pipeline_built\"] = True\n",
    "            st.success(\"Pipeline ready.\")\n",
    "\n",
    "    if \"pipeline_built\" not in st.session_state:\n",
    "        st.info(\"Click 'Build / Load pipeline' in the sidebar to begin.\")\n",
    "        return\n",
    "\n",
    "    # Chat UI\n",
    "    st.subheader(\"Ask a question\")\n",
    "    query = st.text_input(\"Enter your question here:\", \"\")\n",
    "    if st.button(\"Search & Answer\"):\n",
    "        chunks = st.session_state[\"chunks\"]\n",
    "        tfidf_vec = st.session_state[\"tfidf_vec\"]\n",
    "        tfidf_mat = st.session_state[\"tfidf_mat\"]\n",
    "        w2v_model = st.session_state[\"w2v_model\"]\n",
    "        w2v_doc_embs = st.session_state[\"w2v_doc_embs\"]\n",
    "        sbert_model = st.session_state[\"sbert_model\"]\n",
    "        sbert_doc_embs = st.session_state[\"sbert_doc_embs\"]\n",
    "\n",
    "        # 1) Hybrid retrieval\n",
    "        candidate_idxs, candidate_scores, comp_scores = hybrid_retrieval(query, tfidf_vec, tfidf_mat,\n",
    "                                                                         w2v_model, w2v_doc_embs,\n",
    "                                                                         sbert_model, sbert_doc_embs,\n",
    "                                                                         [c[\"content\"] for c in chunks],\n",
    "                                                                         top_k=int(topk_candidates))\n",
    "        candidates = [chunks[i] for i in candidate_idxs]\n",
    "\n",
    "        # 2) Cross-encoder rerank\n",
    "        # load cross-encoder model (on-demand)\n",
    "        cross_enc = st.session_state.get(\"cross_encoder\", None)\n",
    "        if cross_enc is None:\n",
    "            with st.spinner(\"Loading cross-encoder for reranking...\"):\n",
    "                cross_enc = CrossEncoder(CROSS_ENCODER_NAME, device=DEVICE)\n",
    "                st.session_state[\"cross_encoder\"] = cross_enc\n",
    "\n",
    "        cand_texts = [c[\"content\"] for c in candidates]\n",
    "        order, rerank_scores = cross_encoder_rerank(query, cand_texts, cross_enc, top_k=int(topk_final))\n",
    "        final_docs = [candidates[i] for i in order]\n",
    "\n",
    "        # 3) Prepare context for LLM (concatenate top-K passages with citations)\n",
    "        context_pieces = []\n",
    "        for d in final_docs:\n",
    "            citation = f\"(source: {Path(d.get('metadata',{}).get('source','unknown')).name}, page: {d.get('page')})\"\n",
    "            context_pieces.append(f\"{d['content']}\\n\\n{citation}\")\n",
    "        context = \"\\n\\n---\\n\\n\".join(context_pieces)\n",
    "\n",
    "        # 4) LLM generation (using simple OpenAI wrapper)\n",
    "        # Use prompt template\n",
    "        if OPENAI_API_KEY is None:\n",
    "            st.warning(\"OpenAI API key not set in OPENAI_API_KEY env var. LLM step may fail or use default LangChain provider.\")\n",
    "        # Create simple LLM call (no heavy LangChain chain for brevity)\n",
    "        try:\n",
    "            llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "            prompt = f\"Answer this question using only the context below. If answer not present, say 'I don't know'.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "            response = llm(prompt)\n",
    "        except Exception as e:\n",
    "            response = f\"LLM call failed: {e}\"\n",
    "\n",
    "        # Display answer and sources\n",
    "        st.subheader(\"Answer\")\n",
    "        st.write(response)\n",
    "        st.subheader(\"Top Retrieved Passages (after rerank)\")\n",
    "        for i, d in enumerate(final_docs):\n",
    "            st.markdown(f\"**Rank {i+1} — source:** {Path(d.get('metadata',{}).get('source','unknown')).name} | page: {d.get('page')}\")\n",
    "            st.write(d[\"content\"][:800] + (\"...\" if len(d[\"content\"])>800 else \"\"))\n",
    "            st.write(\"---\")\n",
    "\n",
    "        # Show component scores overview for top candidates\n",
    "        st.subheader(\"Candidate Scores (fusion components)\")\n",
    "        df_scores = pd.DataFrame({\n",
    "            \"doc_id\": candidate_idxs,\n",
    "            \"fused_score\": candidate_scores,\n",
    "            \"tfidf_score\": comp_scores[\"tfidf\"][candidate_idxs],\n",
    "            \"w2v_score\": comp_scores[\"w2v\"][candidate_idxs],\n",
    "            \"sbert_score\": comp_scores[\"sbert\"][candidate_idxs]\n",
    "        })\n",
    "        st.dataframe(df_scores.head(20))\n",
    "\n",
    "    # Optional: provide evaluation panel\n",
    "    st.sidebar.header(\"Evaluation\")\n",
    "    if st.sidebar.button(\"Run IR evaluation (build queries per page)\"):\n",
    "        with st.spinner(\"Running evaluation...\"):\n",
    "            # Build queries: pick one representative chunk per page (longest chunk)\n",
    "            chunks = st.session_state[\"chunks\"]\n",
    "            # build page -> indices\n",
    "            page_to_indices = {}\n",
    "            for idx, c in enumerate(chunks):\n",
    "                pg = c.get(\"page\")\n",
    "                if pg is None: continue\n",
    "                page_to_indices.setdefault(pg, []).append(idx)\n",
    "            queries = []\n",
    "            query_label_indices = []\n",
    "            for pg, idxs in page_to_indices.items():\n",
    "                # choose longest chunk\n",
    "                idx_long = max(idxs, key=lambda i: len(chunks[i][\"content\"]))\n",
    "                queries.append(chunks[idx_long][\"content\"].split(\".\")[0][:300])  # first sentence short\n",
    "                query_label_indices.append([idx_long])\n",
    "            # Evaluate using hybrid retrieval (tfidf + w2v + sbert)\n",
    "            all_metrics = []\n",
    "            for qi, q in enumerate(tqdm(queries)):\n",
    "                sims_top = hybrid_retrieval(q, st.session_state[\"tfidf_vec\"], st.session_state[\"tfidf_mat\"],\n",
    "                                           st.session_state[\"w2v_model\"], st.session_state[\"w2v_doc_embs\"],\n",
    "                                           st.session_state[\"sbert_model\"], st.session_state[\"sbert_doc_embs\"],\n",
    "                                           [c[\"content\"] for c in chunks], top_k=TOP_K_CANDIDATES)[2][\"fused\"]\n",
    "                metrics = calculate_ir_metrics_for_query(sims_top, query_label_indices[qi], build_label_matrix(chunks), k=TOP_K_FINAL)\n",
    "                all_metrics.append(metrics)\n",
    "            # average\n",
    "            avg = {k: np.mean([m[k] for m in all_metrics]) for k in all_metrics[0].keys()}\n",
    "            st.success(\"Evaluation finished\")\n",
    "            st.write(avg)\n",
    "\n",
    "# helper to build labels matrix from chunks (page-based)\n",
    "def build_label_matrix(chunks):\n",
    "    pages = sorted(list({c.get(\"page\") for c in chunks if c.get(\"page\") is not None}))\n",
    "    page_to_idx = {p:i for i,p in enumerate(pages)}\n",
    "    labels = np.zeros((len(chunks), len(pages)), dtype=int)\n",
    "    for i,c in enumerate(chunks):\n",
    "        p = c.get(\"page\")\n",
    "        if p is None: continue\n",
    "        labels[i, page_to_idx[p]] = 1\n",
    "    return labels\n",
    "\n",
    "# -----------------------\n",
    "# Entrypoint: run streamlit\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # If run with 'streamlit run', the Streamlit environment will start here.\n",
    "    # For direct python run, start Streamlit programmatically (not recommended).\n",
    "    run_streamlit_app()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
